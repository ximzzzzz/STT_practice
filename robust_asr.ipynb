{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time, math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from warpctc_pytorch import CTCLoss\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "from data.data_loader import AudioDataLoader, SpectrogramDataset, BucketingSampler\n",
    "from data.data_loader import get_accents\n",
    "from decoder import GreedyDecoder\n",
    "from model import DeepSpeech, supported_rnns, ForgetNet, Encoder, Decoder, DiscimnateNet\n",
    "from utils import reduce_tensor, check_loss, Decoder_loss\n",
    "\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    'train_manifest' : './data/csvs/train_sorted_EN_US.csv',\n",
    "    'val_manifest' : './data/csvs/test_sorted_EN_US.csv',\n",
    "    'sample_rate' : 16000,\n",
    "    'labels_path' : 'labels.json',\n",
    "    'window_size' : .02, 'window_stride' : .01, 'window' : 'hamming',\n",
    "    'hidden_size' : 1024, 'hidden_layers' : 5, 'rnn_type' : 'gru',\n",
    "    'epochs' : 50, 'batch_size' : 32, 'num_workers' : 4,\n",
    "    'patience' : 10,\n",
    "    'cuda' : True,\n",
    "    'lr' : 0.001, 'momentum' : 0.9, 'max_norm' : 400, 'learning_anneal' : 1.1, \n",
    "    'silent' : True,\n",
    "    'checkpoint' : True, 'checkpoint_per_batch' : 5000,\n",
    "    'visdom' : True, 'tensorboard' : True,\n",
    "    'log_dir' : './visualize/deepspeech_final', 'log_params' : True,\n",
    "    'id' : 'Deepspeech training',\n",
    "    'continue_from' : '', 'finetune' : True,\n",
    "    'augment' : True,\n",
    "    'noise_dir' : None, 'noise_prob' : 0.4, 'noise_min' : 0.0, 'noise_max' : 0.5,\n",
    "    'no_shuffle' : True,\n",
    "    'no_sorta_grad' : True,\n",
    "    'bidirectional' : True,\n",
    "    'spec_augment' : True,\n",
    "    'dist_url' : 'tcp://127.0.0.1.:1550', 'dist_backend' : 'nccl',\n",
    "    'world_size' : 1, \n",
    "    'rank' : 0,\n",
    "    'enco_modules' : 2, 'enco_res' : True, \n",
    "    'disc_modules' : 1, 'disc_res' : False,\n",
    "    'forg_modules' : 2, 'forg_res' : True,\n",
    "    'gpu_rank' : 0,\n",
    "    'seed' : 123456,\n",
    "    'opt_level' : '',\n",
    "    'keep_batchnorm_fp32' : None,\n",
    "    'loss_scale' : None,\n",
    "    'weights' : ' ',\n",
    "    'update_rule' : 1,\n",
    "    'train_asr' : False,\n",
    "    'dummy' : True,\n",
    "    'num_epochs' : 1,\n",
    "    'mw_alpha' : 0.1, 'mw_beta' : 0.2, 'mw_gamma' : 0.6 ,\n",
    "    'exp_name' : './exp/1224/'\n",
    "\n",
    "})\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "torch.cuda.set_device(int(args.gpu_rank))\n",
    "eps = 0.0000000001 # epsilon value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating accents\n"
     ]
    }
   ],
   "source": [
    "accent_dict = get_accents('./data/csvs/train_sorted_EN_US.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accent = list(accent_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels.json') as label_file:\n",
    "        labels = str(''.join(json.load(label_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"_'ABCDEFGHIJKLMNOPQRSTUVWXYZ \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_conf = dict(sample_rate=16000,\n",
    "                    window_size=.02,\n",
    "                    window_stride=.01,\n",
    "                    window='hamming',\n",
    "                    noise_dir= None,\n",
    "                    noise_prob= 0.4,\n",
    "                    noise_levels=(0.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASR\n",
    "asr = DeepSpeech(rnn_hidden_size=1024,\n",
    "          nb_layers = 5,\n",
    "          rnn_type =nn.GRU,\n",
    "          audio_conf = audio_conf,\n",
    "          bidirectional = True)\n",
    "asr = asr.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train.py --train-manifest data/csvs/train_sorted_EN_US.csv --val-manifest data/csvs/dev_sorted_EN_US.csv --cuda --rnn-type gru --hidden-layers 5 --hidden-size 1024 --epochs 50 --lr 0.001 --batch-size 32 --gpu-rank 0 --update-rule 1 --exp-name ./exp/1224/ --mw-alpha 0.1 --mw-beta 0.2 --mw-gamma 0.6 --enco-modules 2 --enco-res --forg-modules 2 --forg-res --num-epochs 1 --checkpoint-per-batch 5000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {} # All the models with their loss and optimizer are saved in this dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASR\n",
    "asr_optimizer = torch.optim.Adam(asr.parameters(), lr=args.lr, weight_decay=1e-4, amsgrad=True)\n",
    "criterion = CTCLoss()\n",
    "models['predictor'] = [asr, criterion, asr_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER & Decoder\n",
    "\n",
    "encoder = Encoder(num_modules = args.enco_modules, residual_bool = args.enco_res)\n",
    "encoder = encoder.to(device)\n",
    "models['encoder'] = [encoder, None, None]\n",
    "\n",
    "decoder = Decoder()\n",
    "decoder = decoder.to(device)\n",
    "dec_loss = Decoder_loss(nn.MSELoss())\n",
    "\n",
    "ed_optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=args.lr, weight_decay=1e-4, amsgrad=True)\n",
    "models['decoder'] = [decoder, dec_loss, ed_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "if not args.train_asr:\n",
    "    discriminator = DiscimnateNet(classes = len(accent), num_modules = args.disc_modules, residual_bool = args.disc_res)\n",
    "    discriminator = discriminator.to(device)\n",
    "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=args.lr,weight_decay=1e-4,amsgrad=True)\n",
    "    accent_counts = pd.read_csv(args.train_manifest, header=None).iloc[:,[-1]].apply(pd.value_counts).to_dict()\n",
    "    disc_loss_weights = torch.zeros(len(accent)) + eps\n",
    "    for accent_type_f in accent_counts:\n",
    "        if isinstance(accent_counts[accent_type_f], dict):\n",
    "            for accent_type_in_f in accent_counts[accent_type_f]:\n",
    "                if accent_type_in_f in accent_dict:\n",
    "                    disc_loss_weights[accent_dict[accent_type_in_f]] += accent_counts[accent_type_f][accent_type_in_f]\n",
    "    disc_loss_weights = torch.sum(disc_loss_weights) / disc_loss_weights  # [2,2]\n",
    "    dis_loss = nn.CrossEntropyLoss(weight = disc_loss_weights.to(device))\n",
    "    models['discriminator'] = [discriminator, dis_loss, discriminator_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (predictor): DeepSpeech(\n",
       "    (rnns): Sequential(\n",
       "      (0): BatchRNN(\n",
       "        (rnn): GRU(1312, 1024, bidirectional=True)\n",
       "      )\n",
       "      (1): BatchRNN(\n",
       "        (batch_norm): SequenceWise (\n",
       "        BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
       "        (rnn): GRU(1024, 1024, bidirectional=True)\n",
       "      )\n",
       "      (2): BatchRNN(\n",
       "        (batch_norm): SequenceWise (\n",
       "        BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
       "        (rnn): GRU(1024, 1024, bidirectional=True)\n",
       "      )\n",
       "      (3): BatchRNN(\n",
       "        (batch_norm): SequenceWise (\n",
       "        BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
       "        (rnn): GRU(1024, 1024, bidirectional=True)\n",
       "      )\n",
       "      (4): BatchRNN(\n",
       "        (batch_norm): SequenceWise (\n",
       "        BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
       "        (rnn): GRU(1024, 1024, bidirectional=True)\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): SequenceWise (\n",
       "      Sequential(\n",
       "        (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Linear(in_features=1024, out_features=3, bias=False)\n",
       "      ))\n",
       "    )\n",
       "    (inference_softmax): InferenceBatchSoftmax()\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (modules_list): ModuleList(\n",
       "      (0): ModuleDict(\n",
       "        (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv_1): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
       "        (conv_2): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
       "      )\n",
       "      (1): ModuleDict(\n",
       "        (batch_norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv_1): Conv2d(32, 32, kernel_size=(41, 11), stride=(1, 1), padding=(20, 5))\n",
       "        (conv_2): Conv2d(32, 32, kernel_size=(21, 11), stride=(1, 1), padding=(10, 5))\n",
       "      )\n",
       "    )\n",
       "    (hard_tanh): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (trans_conv_1): ConvTranspose2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5), output_padding=(1, 0))\n",
       "    (trans_conv_2): ConvTranspose2d(32, 1, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5), output_padding=(1, 1))\n",
       "    (hard_tanh): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
       "  )\n",
       "  (discriminator): DiscimnateNet(\n",
       "    (adaptive_pooling): AdaptiveAvgPool2d(output_size=(2, 2))\n",
       "    (linear_1): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "    (base): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(21, 11), stride=(1, 1), padding=(10, 5))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (3): Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (6): Conv2d(128, 256, kernel_size=(5, 11), stride=(1, 1), padding=(2, 5))\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (cnns): Sequential(\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(5, 11), stride=(1, 1), padding=(2, 5))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how it looks like\n",
    "nn.Sequential(OrderedDict([ (k, v[0]) for k, v in  models.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderedDict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
